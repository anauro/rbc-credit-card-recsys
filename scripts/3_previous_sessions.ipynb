{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78e8250-89a5-4725-8b53-ded2bca1ee18",
   "metadata": {},
   "source": [
    "## PREVIOUS SESSIONS\n",
    "\n",
    "We are interested to know if prospects ever visited our public site before and if so, whether they had already explored or considered certain credit cards and accounts. \n",
    "*In the initial training of the model, we will be pulling session details for the session during which the prospect first applied for a CC. Once the model is live, this will be pulled real-time for each new prospect.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87357ec6-0d95-46f3-b20a-b136e438d1d4",
   "metadata": {},
   "source": [
    "##### Timing \n",
    "We want to time how long these programs take to run. We are interested both in real time and CPU time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7185dd5c-e228-481e-8cb7-dd800eddd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "start_cpu_time = time.process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a29ebf-fb9a-4016-8149-abf9472c6437",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbd0a6-c689-4ef3-8919-006eb296ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import collect_list, regexp_replace, lower\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import year, month, dayofmonth, to_date, trim, concat, col, lit\n",
    "from functools import reduce\n",
    "\n",
    "import datetime \n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7d3ee-75c0-47cc-b3b3-fa617d441624",
   "metadata": {},
   "source": [
    "#### Function to Create Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398c45d-e238-46fb-bd4c-be91b007ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_end_list(num_months):\n",
    "\n",
    "    #Define period start and end based on today's date \n",
    "    today = datetime.date.today()\n",
    "    period_start = today.replace(day=1) + relativedelta(months=-num_months)\n",
    "    period_end = today.replace(day=1)\n",
    "\n",
    "    #Create list of month ends\n",
    "    dtrange = pd.date_range(start=period_start, end=period_end, freq='d')\n",
    "    months = pd.Series(dtrange.month)\n",
    "    starts, ends = months.ne(months.shift(1)), months.ne(months.shift(-1))\n",
    "    df = pd.DataFrame({'month_starting_date': dtrange[starts].strftime('%Y-%m-%d'),\n",
    "                       'month_ending_date': dtrange[ends].strftime('%Y-%m-%d')})\n",
    "    \n",
    "    me_list = list(df['month_ending_date'])[:-1]\n",
    "    return me_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27013c7-fcfc-455f-9949-5d1b8c245f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_end_list(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbe2ec-8a94-422e-9f67-2e0b50471dc5",
   "metadata": {},
   "source": [
    "#### Relevant Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8073ccf-a4ff-481d-a340-89ea376f8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_of_interest = [\n",
    "    'experience_impression',\n",
    "    'view_promotion',\n",
    "    'first_visit',\n",
    "    'click',\n",
    "    'generate_tool',\n",
    "    'begin_tool',\n",
    "    'select_promotion',\n",
    "    'add_to_cart',\n",
    "    'remove_from_cart',\n",
    "    \"view_item\",\n",
    "    \"select_item\",\n",
    "    \"begin_checkout\",\n",
    "    \"purchase\"\n",
    "]\n",
    "\n",
    "lobs_of_interest = [\n",
    "    'credit cards',\n",
    "    'accounts',\n",
    "    'students'\n",
    "]\n",
    "\n",
    "urls_of_interest = '%newcomers%'\n",
    "\n",
    "bap_regex = \"(british.*airways|/ba/)\"\n",
    "mcp_regex = \"%MCP%\"\n",
    "iav_regex = \"(infinite|iav|cartes/avion|fridayfriendpass)\"\n",
    "ion_regex = \"(ion\\-|/ion/)\"\n",
    "mc4_regex = \"(westjet.*world.*elite|wj|mc4)\"\n",
    "mc1_regex = \"(cash.*back.*mastercard|mc1)\"\n",
    "gcp_regex = \"%avion%platinum%\"\n",
    "gus_regex = \"%us%dollar%visa%gold%\"\n",
    "mv1_regex = \"%moi%\"\n",
    "avp_regex = \"%privilege%\"\n",
    "plt_regex = \"%visa%platinum%\"\n",
    "iop_regex = \"%iop-%\"\n",
    "mc2_regex = \"(westjet.*mastercard|mc2)\"\n",
    "clo_regex = \"(classic.*low.*rate|clo|low\\-interest)\"\n",
    "\n",
    "\n",
    "d2d_regex = '%day-to-day-banking%'\n",
    "adv_regex = '%advantage-banking%'\n",
    "snlb_regex = '%signature-no-limit-banking%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff27d9-1f97-4bd0-b285-dee51d0863f1",
   "metadata": {},
   "source": [
    "##### Load Prospect Base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7ab2e-c730-4a0b-a2ce-0275a5bfaf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = spark.read.load(\"/anaurosevic/cdn0_cards_affinity/prospect_base/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1e0ac-6172-40e8-bcb8-5f8b244e9176",
   "metadata": {},
   "source": [
    "##### Load Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c07f61-c9dd-4e15-a86b-f538ad626ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_string = \"session_date>='\"+str(month_end_list(12)[0])+\"' and session_date<'\"+str(month_end_list(12)[-1])+\"'\"\n",
    "print(filter_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a25435-7166-4f05-a56e-7d122c95cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = spark.read.option(\n",
    "    \"basePath\",\"...GA4_SESSION...\").load(\n",
    "    \"...GA4_SESSION...\").filter(\n",
    "    filter_string).filter(  #remove branch computers \n",
    "    \"ep_traffic_type is null\").withColumn(\n",
    "    \"sess_timestamp\",F.from_unixtime(F.col(\"user_session_start_timestamp\")/1e6)).withColumn(\n",
    "    \"sess_date\",F.to_date(\"sess_timestamp\")).select(\n",
    "    \"user_pseudo_id\",\"user_session_id\",\"sess_date\",\"sess_timestamp\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0614e-7a4b-46c3-876a-01cbf69f2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab first sess_timestamp for each session ID to simplify \n",
    "session_filtered = session.withColumn(\n",
    "     \"rank\", F.row_number().over(Window.partitionBy(\"user_session_id\").orderBy(\"sess_timestamp\"))).filter(\n",
    "    \"rank=1\").drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ec159-d8c6-43bd-a86e-620e4ce95873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab only sessions which occur in the 30 days before application\n",
    "#Only some users have previous sessions - let's subset to this group \n",
    "sessions_30d = pb.join(session_filtered,\n",
    "                       pb.user_pseudo_id == session_filtered.user_pseudo_id, how='inner').drop(\n",
    "    session_filtered.user_session_id).drop(session_filtered.user_pseudo_id).filter(\n",
    "    (F.col('sess_date')<F.col('card_sale_date')) & (F.col('sess_date')>=F.date_sub(F.col('card_sale_date'),30))).select(\n",
    "    'user_pseudo_id','user_session_id','clnt_no','sess_date').persist() #Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a72a83-636e-4db4-b612-28269b51a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sessions_30d.count() \n",
    "#37,379 rows at last update (May 28, 2025) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a629c9-b383-4068-8e74-8928322fca5f",
   "metadata": {},
   "source": [
    "##### Load E-commerce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6573b7b-7602-47db-8b8d-df15ff36db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What files do we already have? Don't duplicate effort :D \n",
    "path = \"/anaurosevic/cdn0_cards_affinity/previous_sessions/events/\"   # Replace with your folder path\n",
    "\n",
    "# Access Hadoop FileSystem\n",
    "hadoop_fs = spark._jsc.hadoopConfiguration()\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_fs)\n",
    "\n",
    "# List subfolders\n",
    "try:\n",
    "    files = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(path))\n",
    "    subfolders = [file.getPath().getName() for file in files if file.isDirectory()]\n",
    "    \n",
    "    if not subfolders:\n",
    "        print(\"No subfolders found in the directory.\")\n",
    "    else:\n",
    "        print(\"Subfolders found:\")\n",
    "        for subfolder in subfolders:\n",
    "            print(subfolder)\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing subfolders: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce34080-3a95-445f-82ab-7fa5637be642",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders #These are the files we have already downloaded :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae06390-48d4-43d2-b90b-1bb40b0ecfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_ecommerce_data(me_list): \n",
    "\n",
    "    #Loop over list of month ends\n",
    "    for i in range(0,len(me_list)): \n",
    "        print(me_list[i])\n",
    "\n",
    "        #Set file name for saving \n",
    "        save_folder_path = \"/anaurosevic/cdn0_cards_affinity/previous_sessions/events/\"\n",
    "        save_file_path =  save_folder_path + str(me_list[i])\n",
    "\n",
    "        #Location of e-commerce files \n",
    "        data_folder_path = \"...GA4_ECOMMERCE...\"\n",
    "        date = dt.strptime(me_list[i], '%Y-%m-%d').date()\n",
    "        date_filter_string = \"YEAR=\" + str(date.strftime('%Y')) + \"/Month=\" + str(date.strftime('%m')) + \"/\"\n",
    "        data_file_path = data_folder_path + date_filter_string\n",
    "\n",
    "        #Import ecommerce data \n",
    "        ecommerce = spark.read.option(\"basePath\",data_folder_path).load(\n",
    "             data_file_path)\n",
    "        #ecommerce = spark.read.load(data_file_path)\n",
    "\n",
    "         #Subset to only clients and sessions of interest\n",
    "        ecommerce_subset = sessions_30d.join(ecommerce, on='user_session_id',how='inner').drop(ecommerce.user_session_id).persist()\n",
    "\n",
    "        #Grab LOBs of interest\n",
    "        previous_ecommerce = ecommerce_subset.filter(\n",
    "            (F.col(\"ep_lob\").isin(lobs_of_interest) & F.col(\"event_name\").isin(events_of_interest)) | F.lower(F.col(\"ep_clean_url\")).like(urls_of_interest))\n",
    "\n",
    "        #Note: Order is important here to prevent regex shenanigans\n",
    "        #Flag events of interest\n",
    "        previous_ecommerce = previous_ecommerce.withColumn(\n",
    "        \"it_item_id\", F.\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").like(avp_regex), \"i_AVP\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").rlike(iav_regex), \"i_IAV\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").rlike(mc4_regex), \"i_MC4\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").rlike(mc2_regex), \"i_MC2\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").rlike(bap_regex), \"i_BAP\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").like(mcp_regex), \"i_MCP\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").rlike(ion_regex), \"i_ION\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").rlike(mc1_regex), \"i_MC1\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").like(gcp_regex), \"i_GCP\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").like(gus_regex), \"i_GUS\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").like(mv1_regex), \"i_MV1\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").like(plt_regex), \"i_PLT\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").like(iop_regex), \"i_IOP\").\\\n",
    "        when(((F.col(\"event_name\") == \"first_visit\") | F.col(\"ep_content_group\").like('credit cards%')) & F.col(\"ep_clean_url\").rlike(clo_regex), \"i_CLP\").\\\n",
    "    \n",
    "        when(F.col(\"ep_content_group\").like(\"%accounts%\") & F.col(\"ep_clean_url\").like(d2d_regex), \"i_022\").\\\n",
    "        when(F.col(\"ep_content_group\").like(\"%accounts%\") & F.col(\"ep_clean_url\").like(adv_regex), \"i_099\").\\\n",
    "        when(F.col(\"ep_content_group\").like(\"%accounts%\") & F.col(\"ep_clean_url\").like(snlb_regex), \"i_004\").\\\n",
    "        otherwise(F.col(\"it_item_id\")))\n",
    "\n",
    "        previous_ecommerce_session_lvl_indicators = previous_ecommerce.withColumn(\n",
    "            \"view\", F.when((F.col(\"event_name\").like('view%') | F.col(\"ep_content_group\").like('credit cards%')) & ~F.col(\"it_item_id\").rlike(\".*00+$\"), F.concat(F.lit(\"view_\"), F.regexp_replace(\"ep_lob\", \"\\s\", \"_\"), F.lit(\"_\"), F.upper(F.split(\"it_item_id\", \"_\").getItem(1))))\n",
    "            ).withColumn(\n",
    "                \"select\", F.when(F.col(\"event_name\").like('select%') & ~F.col(\"it_item_id\").rlike(\".*00+$\"), F.concat(F.lit(\"select_\"), F.regexp_replace(\"ep_lob\", \"\\s\", \"_\"), F.lit(\"_\"), F.upper(F.split(\"it_item_id\", \"_\").getItem(1))))\n",
    "            ).withColumn(\n",
    "                \"checkout\", F.when(F.col(\"event_name\").like('begin_checkout') & ~F.col(\"it_item_id\").rlike(\".*00+$\"), F.concat(F.lit(\"checkout_\"), F.regexp_replace(\"ep_lob\", \"\\s\", \"_\"), F.lit(\"_\"), F.upper(F.split(\"it_item_id\", \"_\").getItem(1))))\n",
    "            ).withColumn(\n",
    "                \"newcomer_view\", F.when(F.col(\"ep_clean_url\").like(urls_of_interest), 1).otherwise(0)\n",
    "            ).withColumn(\n",
    "                \"student_view\", F.when((F.col(\"ep_lob\") == 'students') | F.col(\"ep_clean_url\").like(\"%student%\") | F.col(\"ep_content_group\").like(\"%student%\"), 1).otherwise(0)\n",
    "            ).withColumn(\n",
    "                \"purchase\", F.when(F.col(\"event_name\").like('begin_checkout') & ~F.col(\"it_item_id\").rlike(\".*00+$\"), F.concat(F.lit(\"checkout_\"), F.regexp_replace(\"ep_lob\", \"\\s\", \"_\"), F.lit(\"_\"), F.upper(F.split(\"it_item_id\", \"_\").getItem(1))))\n",
    "            ).persist() \n",
    "    \n",
    "        #(A) Select event\n",
    "        previous_ecommerce_selections = previous_ecommerce_session_lvl_indicators.groupBy(\"user_session_id\").pivot(\n",
    "            \"select\").agg(F.lit(1)).fillna(0).drop(\"null\")\n",
    "    \n",
    "        #(B) View event\n",
    "        previous_ecommerce_views = previous_ecommerce_session_lvl_indicators.groupBy(\"user_session_id\").pivot(\n",
    "            \"view\").agg(F.lit(1)).fillna(0).drop(\"null\")\n",
    "    \n",
    "        #(C) Checkout event\n",
    "        previous_ecommerce_checkouts  = previous_ecommerce_session_lvl_indicators.groupBy(\"user_session_id\").pivot(\n",
    "            \"checkout\").agg(F.lit(1)).fillna(0).drop(\"null\")\n",
    "    \n",
    "        #(D) Demo event\n",
    "        previous_ecommerce_demos = previous_ecommerce_session_lvl_indicators.groupBy(\"user_session_id\").agg(\n",
    "            F.max(\"newcomer_view\").alias(\"newcomer_view\"), F.max(\"student_view\").alias('student_view'))\n",
    "    \n",
    "        #Final\n",
    "        previous_ecommerce_session_lvl = previous_ecommerce_selections.join(previous_ecommerce_views, ['user_session_id'], 'full_outer').join(\n",
    "            previous_ecommerce_checkouts, ['user_session_id'], 'full_outer').join(\n",
    "            previous_ecommerce_demos, ['user_session_id'], 'full_outer').fillna(0)\n",
    "                    \n",
    "        previous_ecommerce_session_lvl.coalesce(1).write.mode(\"overwrite\").parquet(save_file_path)\n",
    "    \n",
    "    return ecommerce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16543a-ccf3-44e8-8374-e6dcc2e70f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_end_list(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b62852-f1c0-465b-a98d-b6e75f286f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_to_download = list(set(month_end_list(12)) - set(subfolders))\n",
    "print(months_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a4738-8fa2-4711-9858-c01384ef042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_ecommerce_data(months_to_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554359ac-8856-4489-930d-63569f5000e6",
   "metadata": {},
   "source": [
    "--- END PROGRAM --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b51a81-8db7-4d36-ae3f-4384980f6918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timing summary\n",
    "end_time = time.time()\n",
    "end_cpu_time = time.process_time()\n",
    "\n",
    "real_time_elapsed = end_time - start_time\n",
    "cpu_time_elapsed = end_cpu_time - start_cpu_time\n",
    "\n",
    "print(f\"Real time: {real_time_elapsed:.2f} seconds\")\n",
    "print(f\"CPU time: {cpu_time_elapsed:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
